{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zero-Sik/Py_ML_Deep_Learning/blob/master/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P27kCx2unVVz",
        "colab_type": "text"
      },
      "source": [
        "# 텍스트 전처리(Text preprocessing)\n",
        "\n",
        " - 자연어 처리에 있어서 텍스트 전처리는 매우 중요한 작업이다.\n",
        " - 텍스트 전처리는 용도에 맞게 텍스트를 사전에 처리하는 작업(요리에 있어서 재료를 제대로 손질 하지 않으면, 요리가 제대로 되지 않는 것과 같다.).\n",
        " - 텍스트에 대해서 제대로 된 전처리를 하지 않으면 뒤에서 배울 자연어 처리 기법들이 제대로 동작 안함.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWDX869xvCyS",
        "colab_type": "text"
      },
      "source": [
        "## 토큰화(Tokenization)\n",
        " - 자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태라면, 해당 데이터를 사용하고자하는 용도에 맞게 토큰회(tokenization) & 정제(cleaning) & 정규화(normalization)하는 일을 하게 됨.\n",
        "\n",
        " - 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 부름.\n",
        " - 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의함.\n",
        " - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzTmwr1SxlNu",
        "colab_type": "text"
      },
      "source": [
        "### 단어 토큰화(Word Tokenization)\n",
        " - 단어(Word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주됨.\n",
        " - 예) 아래의 입력으로부터 구두점(punctuation)과 같은 문자는 제외 시키는 간단한 단어 토큰화 작업 실시.\n",
        "  -구두점이란, 은점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호.\n",
        "  - 입력 : Time is an illusion. Lunchtime double so!\n",
        "   - 이러한 입력으로부터 구두점을 제외시킨 토큰화 작업의 결과는 다음과 같다.\n",
        "    - 출력 : \"Time\", \"is\", \"an\", \"illusion\", 'double', \"so\"\n",
        "\n",
        "  - 예제에서 토큰화 작업은 굉장히 간단하며, 구두점을 지운 뒤에 띄어쓰기(whitespace)를 기준으로 잘라냄.\n",
        "\n",
        "\n",
        " - 특수문자를 전부 제거하는 정제(cleaning) 작업을 수행하는 것만으로 해결되지 않음.\n",
        " - 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 토큰화 중 생기는 선택의 순간\n",
        " - 토큰화의 기준\n",
        "  - 해당 데이터를 가지고 어떤 용도로 사용할 것인지에 따라, 그 용도에 영향이 없는 기준으로 정하면 된다.\n",
        "  - 예) 영어권 언어에서 아포스트로피를(')가 들어가있는 단어는 어떻게 토큰으로 분류해야될까라는 문제\n",
        "\n",
        "   - 예) Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\n",
        "   아포스트로피가 들어간 상황에서 Don't와 Jone's는 어떻게 토큰화 시킴?\n",
        "\n",
        "\n",
        "   - Don't\n",
        "   - Don t\n",
        "   - Dont\n",
        "\n",
        "   - Do n't\n",
        "\n",
        "   Jone's\n",
        "   - Jone s\n",
        "   - Jone\n",
        "   - Jones\n",
        "\n",
        "\n",
        "  - 원하는 결과가 나오도록 토큰화 도구를 직접 설계할 수도 있지만, 기존에 공개된 도구들을 사용하였을 때의 결과가 사용자의 목적과 일치한다면 해당 도구를 사용할 수 도 있을 것임.\n",
        "  - NLTK는 영어 코퍼스를 토큰화하기 위한 도구를 제공.\n",
        "  - 그 중 work_tokenize와 WordPunctTokenizer를 사용해서 NLTK에서는 아포스트로피를 어덯게 처리하는지 확인.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXBV0m9B3JM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6bce7100-f5fd-42f6-c3dc-a2267ad41e33"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlQytE_M4fNQ",
        "colab_type": "text"
      },
      "source": [
        "- word_tokenize는 Don't를 Do와 n't로 분리하였으며, 반면 Jone's는 Jone과 's로 분리한 것을 확인."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMU9T9Kr4Lzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "08d32fde-67b9-419b-d2f7-a5e0f8827bc3"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer  #wordPunctTokenizer\n",
        "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyZdyzOj5V2T",
        "colab_type": "text"
      },
      "source": [
        "- WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기 때문에, 앞서 확인했던 word_tokenize와는 달리 Don't를 Don과 '와 t로 분리함.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27eMQYre5hsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "34c2a533-8740-46a2-c6db-151e47f2d116"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgcfwceH5sy9",
        "colab_type": "text"
      },
      "source": [
        "- 케라스의 text_to_word_sequence는 기본적으로 모든 알파뱃을 소문자로 바꾸면서 온점이나 컴마, 느낌표 등의 구두점을 제거. 하지만 don't나 'jone's와 같은 경우 아포스트로피는 보존.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDq6_pBt55Ts",
        "colab_type": "text"
      },
      "source": [
        "### 토큰화에서 고려해야할 사항\n",
        " - 토큰화 작업을 단순하게 코퍼스에서 구두점을 제외하고 공백 기준으로 잘라내는 작업이라고 간주할 수 없다.\n",
        " - 이러한 일은 섬세한 알고리즘이 필요한데, 왜 섬세해야하는지 그 이유를 정리할 것.\n",
        "\n",
        "\n",
        "  - 1) 구두점이나 특수 문자를 단순 제외해서는 안 된다.\n",
        "   - 갖고 있는 코퍼스에서 단어들을 걸러낼 떄, 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않다.\n",
        "   -코퍼스에 대한 정제 작업을 진행하다보면 구두점조차도 하나의 토큰으로 분류함.\n",
        "  \n",
        "\n",
        "  - 2) 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
        "   - 토큰화 작업에서 종종 영어권 언어의 아포스트로피(')는 압축된 단어를 다시 펼치는 역할을 하기도 한다.\n",
        "   - 예) what're는 what are의 줄임말. we're는 we are의 줄임말. \n",
        "   - 토큰화 작업은 저러한 단어를 하나로 인식할 수 있는 능력도 가져야함.\n",
        "   \n",
        "\n",
        "  - 3) 표준 토큰화 예제\n",
        "   - 이해를 돕기 위해, 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treeback Tokenization의 규칙\n",
        "\n",
        "\n",
        "    - 규칙 1. 하이푼으로 구성된 단어는 하나로 유지.\n",
        "    - 규칙 2. doesn't와 같이 아포스트로피 '접어'가 함께하는 단어는 분리\n",
        "   \n",
        "    - 해당 표준에 아래의 문장을 input을 넣어줌\n",
        "   \n",
        "   - \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yQqBV1h-Qa6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "22082edb-95ab-4f7a-ef9e-5ec72603a1c9"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print(tokenizer.tokenize(text))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNQjlTE--aRt",
        "colab_type": "text"
      },
      "source": [
        "- 결과를 보면, 각각 규칙1과 규칙2에 따라서 horm-based는 하나의 토큰으로 취급하고 있으며, dosen't의 경우 dose와 n't는 분리 되었음."
      ]
    }
  ]
}