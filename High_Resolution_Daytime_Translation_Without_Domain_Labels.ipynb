{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "High_Resolution_Daytime_Translation_Without_Domain_Labels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkZP7+6ufajebFJwEd0g1E"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8eqBH7wdi9A"
      },
      "source": [
        "Set up the environment\n",
        "\n",
        "Don't torgert to chang the runtime type to GPU if using google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMSmbkkrZG3I",
        "outputId": "d74d63de-62d2-4649-ff11-0cb6bc46f8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "\n",
        "!git clone https://github.com/saic-mdal/HiDT.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'HiDT'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 374 (delta 1), reused 4 (delta 1), pack-reused 369\u001b[K\n",
            "Receiving objects: 100% (374/374), 173.69 MiB | 17.44 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enl2BmTIdsD2"
      },
      "source": [
        "#Load images, initialise generator network\n",
        "\n",
        "\n",
        "\n",
        "- set up networks for the conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNTp5_wKFW75"
      },
      "source": [
        "\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('./HiDT')\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "from hidt.networks.enhancement.RRDBNet_arch import RRDBNet\n",
        "from hidt.style_transformer import StyleTransformer\n",
        "from hidt.utils.preprocessing import GridCrop, enhancement_preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDgOe2WGaZxW"
      },
      "source": [
        "config_path = './HiDT/configs/daytime.yaml'\n",
        "gen_weights_path = './HiDT/trained_models/generator/daytime.pt'\n",
        "inference_size = 256  # the network has been trained to do inference in 256px, any higher value might lead to artifacts\n",
        "device = 'cuda:0'\n",
        "image_path = './HiDT/images/daytime/content/1.jpg'\n",
        "styles_path = './HiDT/styles.txt'\n",
        "enhancer_weights = './HiDT/trained_models/enhancer/enhancer.pth'\n",
        "\n",
        "style_transformer = StyleTransformer(config_path,\n",
        "                                     gen_weights_path,\n",
        "                                     inference_size=inference_size,                                      device=device)\n",
        "with open(styles_path) as f:\n",
        "    styles = f.read()\n",
        "styles = {style.split(',')[0]: torch.tensor([float(el) for el in style.split(',')[1][1:-1].split(' ')]) for style in styles.split('\\n')[:-1]}\n",
        "image = Image.open(image_path)\n",
        "crop_transform = GridCrop(4, 1, hires_size=inference_size * 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiqdDS4Gd3An"
      },
      "source": [
        "#Original image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLX0ixC3d1T3"
      },
      "source": [
        "\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F14lVaXd5-9"
      },
      "source": [
        "# Choose style vector you want to transfer to\n",
        "\n",
        "We have provided styles.txt file with eight styles that we found nice and meaningfull You can find the full list of styles below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y9dLwBceFkO"
      },
      "source": [
        "print(list(styles.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecwimymBeHeO"
      },
      "source": [
        "# Select the style, or define any vector you want\n",
        "style_to_transfer = styles['sunset_hard_harder']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT-hQWiueJYW"
      },
      "source": [
        "style_to_transfer = style_to_transfer.view(1, 1, 3, 1).to(device)\n",
        "with torch.no_grad():\n",
        "    content_decomposition = style_transformer.get_content(image)[0]\n",
        "    decoder_input = {'content': content_decomposition['content'],\n",
        "                     'intermediate_outputs': content_decomposition['intermediate_outputs'],\n",
        "                     'style': style_to_transfer}\n",
        "    transferred = style_transformer.trainer.gen.decode(decoder_input)['images']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcmGquzteRuu"
      },
      "source": [
        "transforms.ToPILImage()((transferred[0].cpu().clamp(-1, 1) + 1.) / 2.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPc5uWqeeSxu"
      },
      "source": [
        "\n",
        "enhancer = RRDBNet(in_nc=48, out_nc=3, nf=64, nb=5, gc=32).to(device)\n",
        "enhancer.load_state_dict(torch.load(enhancer_weights))\n",
        "with torch.no_grad():\n",
        "    crops = [img for img in crop_transform(image)]\n",
        "    content_decomposition = style_transformer.get_content(crops)[0]\n",
        "    decoder_input = {'content': content_decomposition['content'],\n",
        "                     'intermediate_outputs': content_decomposition['intermediate_outputs'],\n",
        "                     'style': style_to_transfer.repeat(16, 1, 1, 1)}\n",
        "    transferred = style_transformer.trainer.gen.decode(decoder_input)['images']\n",
        "    padded_stack = enhancement_preprocessing(transferred, normalize=False)\n",
        "    out = enhancer(padded_stack)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrW7P4ZaeV3-"
      },
      "source": [
        "\n",
        "transforms.ToPILImage()((out.cpu()[0].clamp(-1, 1) + 1.) / 2.)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}